\section{Introduction}
Modern advancements in artifical neural networks (ANNs) have showed great potential as a viable solution for a wide variety of problems such as image and speech recognition, prediction and classification. 

By building complex networks of interconnected neurons, the Hopfield network is able to store and recall multiple patterns, and is therefore an important model for understanding the mechanism behind associative memory for humans and animals. While the Hopfield network may not be the most used in the field of ANNs, use-cases do exist such as image reconstruction. 

As humans we have the ability to recall memories from partial information, such as from incomplete images or by observing similar patterns. An apple is an apple, independent of its color or excact shape. The Hopfield Network may help us understand how it is possible to either restore lost information from partial inputs, or how we the brain is able to recall memories by association.

By borrowing the concept of energy and work from physics, we will create a physical interpretation of the Hopfield network and use this to explain the otherwise complex dynamics. We will investigate the dynamics and stability properties of the Hopfield network, using both numerical and analytical methods. We show how storing memories creates local equilibriums where the network remains stable, allowing the network to restore multiple memories using a predefined set of network weights based on Hebbian learning. We then investigate how each stored pattern coincides with two equilibrium points.