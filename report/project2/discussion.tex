\section{Discussion and conclusion}

From our empirical results we are able to verify the stability properties of the Hopfield network. The network is able to restore multiple different patterns from partial inputs, and is stable in the equlibrium points. Our empirical results cannot guarantee global stability, and it is difficult to draw good conclusions purely from a few simulated results. There are however a great deal of research on the stability of the Hopfield network, and the stability of the network have been mathematically proved using Lyapunov analysis. By considering the energy in \cref{eq:energy} as a Lyapunov candidate, it can be show that the energy is (nonstrictly) decreasing when using the update rule in \cref{eq:update} \cite{lyapnuv-stability}.

When simulating with multiple uncorrelated patterns the network energy lower limit from \cref{eq:energy-limits} appears to be roughly divided equally between the two patterns. The energy limits may therefore not only be used understand how the network is stable, but can also be used to quantify the capacity (i.e. the number of memories the network can store). How the energy is distributed does however depend on how similar the memories are. We can think of memories with a lot in common as creating smaller valleys within a larger valley, and thereby sharing some potential energy. Intuitively we should therefore be able to store as many memories as we can create valleys using the finite amount of discrete energy in the network.

The energy may also be used to quantify the robustness of the network. By considering the amount of work needed (in a physical sense) to move the state of the network from one memory to another, we can create a quantitative measure for how robust the network is to random perturbations. If the amount of energy required to move the state between two memories (i.e. the amount of work required to move the state out of one valley) is small, then noise in only a few bits may be enough to recall the wrong memory. Storing too many memories (i.e. having too many small valleys), or storing very similar patterns will therefore decrease the robustness.

Our results also showed that for each stored pattern, there are two equlibrium points - an original and an inverted pattern. In other words, the Hopfield network does not store the actual values of the stored patterns, but only whether any pair of neurons should be equal or not. This appears to be consistent with how we as humans are able to associate memories with objects regardless of specific details - we can recognize a number regardless of its color or background.

There are however many limitations to the Hopfield network, and it is in no way a perfect model of the mechanisms behind memory in humans or animals. Nevertheless it helps us understand how a complex network of simple units (neurons) are able to recall memories, and many of its limitations have been solved in more modern approaches to ANNs.







