\section{Method}

\subsection{Dataset}
The datasets originate from a study on the mechanism behind head direction sense \cite{projectdata}.
Five adult male and two female mice were implanted with recording electrodes under anesthesia \cite{projectdata}.
Silicon probes were mounted on movable drives for recording of neural activity in the anteriour thalamus in all mice, while in additional three of the mice probes were also mounted in the post-subiculum (PoS) \cite{projectdata}. For the remaning four mice with probes only mounted in the anteriour thalamus, additional probes were mounted in the hippocampal CA1 pyramidal layer for accurate sleep scoring \cite{projectdata}.
The head directions (HD) were tracked using two light-emitting-diodes (LEDs) mounted to the back of the head and recored using a videocamera with a frame rate of 30fps. The data was then resampled to 39Hz by the aquisition system \cite{projectdata}.
Data from two mice were explored, mouse 12 with probes only in the anteriour thalamus and mouse 28 with additional probes in the PoS. Only the awake dataset where used.
\subsection{Tuning curves}
The electrode spikes can be encoded as timestamps synchronized with the tracked head direction. The timestamps is binned into timebins with a time resolution of 
$$\Delta t = \frac{1}{39Hz} \approx 25.64ms$$ 
corresponding to the sampling frequency of $39$Hz set by the aquisition system.
Each bin corresponds to the number of cell firings over a period of $\Delta t$ time, and each sample has a corresponding head angle.
Dividing by the sampling time yields the number of spikes per unit time in each timebin.

The continious range of possible head directions, 
$\theta \in [0, 2\pi)$
, can be discretized into a desired number of edges $N_{HD}$ with fixed spacing such that
\begin{equation}
\theta_k \in \{\frac{2 \pi k}{N_{HD}}| k \in \mathbb{N}, 0 \leq k \leq N_{HD} \}
\end{equation}
and each tracked head direction sample is binned according to the rule
\begin{equation}
    f_{\theta}(\theta) =  \begin{cases}
        \theta_0, & \text{if } \theta_0 \leq \theta < \theta_{1} \\
        \theta_1, & \text{if } \theta_1 \leq \theta < \theta_{2} \\
         & \vdots \\
        \theta_k, & \text{if } \theta_k \leq \theta < \theta_{k+1}
    \end{cases} \quad \text{for} \quad 0 \leq k < N_{HD}
\end{equation}
The number of cell firings were binned accordingly to create an overview of number of spikes per head angle.
The firing rate can be found by dividing the number of spikes by the number of head direction samples for each bin.
\begin{equation} \label{eq:firing_rate}
    \lambda_k = \frac{\sum_i S_i}{\sum_i 1} \quad i \in \{t | t \in \mathbb{N}, 0 \leq t \leq N, f_\theta(\theta_t) = \theta_k\}
\end{equation}
where $S_i$ is the number of spikes in timestep $i$, $\theta_t$ is the head angle in timestep $t$ and $N$ is the number of timesteps. Tuning curves are created by plotting the firing rate $\lambda_k$ against the head direction $\theta_k$ in MATLAB or other similar tools.

\subsection{Mutual information}
Mutual information can be used to quantify how much knowing the state of one variable can tell us about another, i.e. how much information is shared between the variables.
The amount of information is quantified as bits (or Shannons), and originates from information theory. A bit can be thought of as the answer to a yes/no question. 
The mutual information can be calculated using 
\begin{equation}\label{eq:mutual_info}
    I(X;Y) = \sum_{i, j} Pr(y_i,x_j) \log_2(\frac{Pr(y_i, x_j)}{Pr(y_i)Pr(x_j)}) = \sum_{i,j}Pr(y_i|x_j)Pr(x_j) \log_2(\frac{Pr(y_i | x_j)Pr(x_j)}{Pr(y_i)Pr(x_j)})
\end{equation}
During a sufficiently short period of time $\Delta t$ the cell spikes can be thought of as a binary (bernouilly) variable, either spiking once or not at all. \cite{mutualinfo}
\begin{align} \label{eq:probs}
    Pr(S=1 | X = k) &= \lambda_k \Delta t & Pr(S=1) = \lambda \Delta t = \Delta t \sum_k \lambda_k
\end{align}
where $\lambda_k$ is the mean fire rate in bin $k$ and $\lambda$ is the mean firing rate over all bins.
By inserting \cref{eq:probs} into \cref{eq:mutual_info} it can be shown that the mutal information can be calculated using a discrete approximation \cite{mutualinfo}.
\begin{equation} \label{eq:mutinfo_disc}
    I \approx \Delta t \sum_k \lambda_k \log_2(\frac{\lambda_k}{\lambda})Pr(X = k)
\end{equation}
It can be rewritted to compute the mutual information per unit time, which may be easier to interpret.
\begin{equation} \label{eq:mutinfo_disc_per_time}
    \frac{I}{\Delta t} \approx \sum_k \lambda_k \log_2(\frac{\lambda_k}{\lambda})Pr(X = k)
\end{equation}
The prior distribution of head angles can be estimated by
\begin{equation}
    \hat{Pr}(X = k) = \frac{\sum_i 1}{N}  \quad i \in \{t | t \in \mathbb{N}, 0 \leq t \leq N, f_\theta(\theta_t) = \theta_k\}
\end{equation}
i.e. the proportion of samples corresponding to bin $k$.

Due to the approximations, \cref{eq:mutinfo_disc_per_time} may lead to slight negative values. Any values below zero is therefore considered to be approximation errors, and is manually forced to zero.

\subsection{Principal Component Analysis}
To reduce the feature space from many potentially unimportant variables to only a few important ones, principal analysis can be used.
The goal of PCA is to create new variables from linear transformations of the original variables by creating a new basis for the vector space. More specifically the goal is to create new variables which describe as much of the variance in the original data as possible.
The amount of variance explained by each variable is sorted descendingly with the first principal component explaining the most, and where each new variables are orthogonal to the rest. MATLAB can be used to perform PCA analysis by giving it a $NxP$ matrix where $N$ is the number of samples and $P$ is the number of features. The output PCA \texttt{scores} contains the original samples represented by the new basis.  
A color coded scatter plot can be generated from the first few principal components. 

